{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4 lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Set, Tuple\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import robotparser\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CrawlScope:\n",
    "    allowed_domain: str               # e.g. \"orfalea.calpoly.edu\"\n",
    "    allowed_path_prefix: str          # e.g. \"/graduate-programs/\"\n",
    "\n",
    "\n",
    "SKIP_EXTENSIONS = (\n",
    "    \".pdf\", \".png\", \".jpg\", \".jpeg\", \".gif\", \".svg\", \".webp\",\n",
    "    \".zip\", \".rar\", \".7z\", \".mp4\", \".mov\", \".mp3\", \".wav\",\n",
    "    \".doc\", \".docx\", \".ppt\", \".pptx\", \".xls\", \".xlsx\"\n",
    ")\n",
    "\n",
    "def normalize_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize URLs to reduce duplicates:\n",
    "    - drop fragments (#...)\n",
    "    - drop query string (optional; here we drop it)\n",
    "    - normalize trailing slash (keep as-is but remove redundant)\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    parsed = parsed._replace(fragment=\"\", query=\"\")\n",
    "\n",
    "    # Normalize netloc to lower\n",
    "    netloc = parsed.netloc.lower()\n",
    "\n",
    "    # Normalize path: collapse // and remove trailing slash except root\n",
    "    path = re.sub(r\"/{2,}\", \"/\", parsed.path)\n",
    "    if path != \"/\" and path.endswith(\"/\"):\n",
    "        path = path[:-1]\n",
    "\n",
    "    parsed = parsed._replace(netloc=netloc, path=path)\n",
    "    return urlunparse(parsed)\n",
    "\n",
    "\n",
    "def is_in_scope(url: str, scope: CrawlScope) -> bool:\n",
    "    p = urlparse(url)\n",
    "    if p.scheme not in (\"http\", \"https\"):\n",
    "        return False\n",
    "    if p.netloc.lower() != scope.allowed_domain.lower():\n",
    "        return False\n",
    "    return p.path.startswith(scope.allowed_path_prefix)\n",
    "\n",
    "\n",
    "def looks_like_file(url: str) -> bool:\n",
    "    lower = url.lower()\n",
    "    return any(lower.endswith(ext) for ext in SKIP_EXTENSIONS)\n",
    "\n",
    "\n",
    "def get_robots_parser(base_url: str) -> robotparser.RobotFileParser:\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    robots_url = urljoin(base_url, \"/robots.txt\")\n",
    "    rp.set_url(robots_url)\n",
    "    try:\n",
    "        rp.read()\n",
    "    except Exception:\n",
    "        # If robots can't be read, default to allowing (your choice).\n",
    "        pass\n",
    "    return rp\n",
    "\n",
    "\n",
    "def fetch_html(session: requests.Session, url: str, timeout: int = 20) -> Optional[str]:\n",
    "    try:\n",
    "        resp = session.get(url, timeout=timeout, headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (compatible; RAG-Crawler/1.0; +https://example.com/bot)\"\n",
    "        })\n",
    "        resp.raise_for_status()\n",
    "        ctype = resp.headers.get(\"Content-Type\", \"\")\n",
    "        if \"text/html\" not in ctype:\n",
    "            return None\n",
    "        return resp.text\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_text_and_title(html: str) -> Tuple[str, str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Remove common boilerplate containers\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    title = (soup.title.get_text(strip=True) if soup.title else \"\").strip()\n",
    "\n",
    "    # Try to focus on main content if present\n",
    "    main = soup.find(\"main\")\n",
    "    container = main if main else soup\n",
    "\n",
    "    text = container.get_text(separator=\"\\n\")\n",
    "    lines = [ln.strip() for ln in text.splitlines()]\n",
    "    # Remove empty/very short lines to reduce noise\n",
    "    lines = [ln for ln in lines if len(ln) >= 30]\n",
    "    cleaned = \"\\n\".join(lines)\n",
    "    return cleaned, title\n",
    "\n",
    "\n",
    "def extract_links(current_url: str, html: str) -> Set[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    links = set()\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "\n",
    "        # Skip mailto/tel/javascript\n",
    "        if href.startswith((\"mailto:\", \"tel:\", \"javascript:\")):\n",
    "            continue\n",
    "\n",
    "        abs_url = urljoin(current_url, href)\n",
    "        abs_url = normalize_url(abs_url)\n",
    "        links.add(abs_url)\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def safe_filename_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Turn a URL into a stable filename.\n",
    "    \"\"\"\n",
    "    p = urlparse(url)\n",
    "    path = p.path.strip(\"/\").replace(\"/\", \"__\")\n",
    "    if not path:\n",
    "        path = \"root\"\n",
    "    return f\"{path}.json\"\n",
    "\n",
    "\n",
    "def crawl_site(\n",
    "    seed_url: str,\n",
    "    scope: CrawlScope,\n",
    "    out_dir: str = \"crawl_output\",\n",
    "    max_pages: int = 500,\n",
    "    delay_seconds: float = 0.5,\n",
    "    respect_robots: bool = True,\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    seed = normalize_url(seed_url)\n",
    "    base = f\"{urlparse(seed).scheme}://{urlparse(seed).netloc}/\"\n",
    "\n",
    "    rp = get_robots_parser(base) if respect_robots else None\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    visited: Set[str] = set()\n",
    "    queue = deque([seed])\n",
    "\n",
    "    pages_saved = 0\n",
    "\n",
    "    while queue and pages_saved < max_pages:\n",
    "        url = queue.popleft()\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        if not is_in_scope(url, scope):\n",
    "            continue\n",
    "        if looks_like_file(url):\n",
    "            continue\n",
    "        if rp and not rp.can_fetch(\"*\", url):\n",
    "            print(f\"ROBOTS SKIP: {url}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"FETCH: {url}\")\n",
    "        html = fetch_html(session, url)\n",
    "        if not html:\n",
    "            continue\n",
    "\n",
    "        text, title = extract_text_and_title(html)\n",
    "\n",
    "        # Save JSON\n",
    "        doc = {\n",
    "            \"source_url\": url,\n",
    "            \"title\": title,\n",
    "            \"content\": text,\n",
    "        }\n",
    "        fname = safe_filename_from_url(url)\n",
    "        fpath = os.path.join(out_dir, fname)\n",
    "        with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(doc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        pages_saved += 1\n",
    "\n",
    "        # Discover more links\n",
    "        for link in extract_links(url, html):\n",
    "            if link not in visited and is_in_scope(link, scope) and not looks_like_file(link):\n",
    "                queue.append(link)\n",
    "\n",
    "        time.sleep(delay_seconds)\n",
    "\n",
    "    print(f\"\\nDone. Visited={len(visited)} Saved={pages_saved} OutputDir={out_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SEED = \"https://orfalea.calpoly.edu/graduate-programs/ms-business-analytics\"\n",
    "    SCOPE = CrawlScope(\n",
    "        allowed_domain=\"orfalea.calpoly.edu\",\n",
    "        allowed_path_prefix=\"/graduate-programs/\"\n",
    "    )\n",
    "\n",
    "    crawl_site(\n",
    "        seed_url=SEED,\n",
    "        scope=SCOPE,\n",
    "        out_dir=\"orfalea_graduate_programs\",\n",
    "        max_pages=300,\n",
    "        delay_seconds=0.7,\n",
    "        respect_robots=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55314f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "INPUT_DIR = \"orfalea_graduate_programs\"\n",
    "OUTPUT_DIR = \"orfalea_graduate_programs_txt\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for fname in os.listdir(INPUT_DIR):\n",
    "    if not fname.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(INPUT_DIR, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = json.load(f)\n",
    "\n",
    "    title = doc.get(\"title\", \"\").strip()\n",
    "    url = doc.get(\"source_url\", \"\").strip()\n",
    "    content = doc.get(\"content\", \"\").strip()\n",
    "\n",
    "    txt = f\"\"\"Title: {title}\n",
    "URL: {url}\n",
    "\n",
    "{content}\n",
    "\"\"\"\n",
    "\n",
    "    out_name = fname.replace(\".json\", \".txt\")\n",
    "    with open(os.path.join(OUTPUT_DIR, out_name), \"w\", encoding=\"utf-8\") as out:\n",
    "        out.write(txt)\n",
    "\n",
    "print(\"Conversion complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
